{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "np.random.seed(32)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras import InputSpec, Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "2bd02ba6-6d16-4cf6-93eb-11ddaf24ed75",
    "_uuid": "4f93b1e1cbd37285475dce9e4f93763e15c3d569"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"C:\\Users\\Lenovo\\projects\\toxic-comments\\datasets\\github\\train.csv\")\n",
    "test = pd.read_csv(r\"C:\\Users\\Lenovo\\projects\\toxic-comments\\datasets\\github\\results.csv\")\n",
    "\n",
    "embedding_path = r\"C:\\Users\\Lenovo\\projects\\toxic-comments\\glove.840B.300d.txt\"\n",
    "embed_size = 300\n",
    "max_features = 100000\n",
    "max_len = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "2ba12a26-1ca9-4ab5-a991-ad822563330e",
    "_uuid": "48b9d3697f49f75aa193c6b0b7256fed1a1db4a4"
   },
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "train[\"comment_text\"].fillna(\"no comment\")\n",
    "test[\"comment_text\"].fillna(\"no comment\")\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_rem, Y_train, Y_rem = train_test_split(train, y, test_size=0.4, random_state=42)\n",
    "X_valid, X_test, Y_valid, Y_test = train_test_split(X_rem, Y_rem, test_size=0.5, random_state=42)\n",
    "X_test1=X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "e5c5937c-535f-4c82-846a-3e121c90f8ef",
    "_uuid": "e725dcb8ff0e354bb5b9afa7c6089d6590c6540f"
   },
   "outputs": [],
   "source": [
    "raw_text_train = X_train[\"comment_text\"].str.lower()\n",
    "raw_text_valid = X_valid[\"comment_text\"].str.lower()\n",
    "raw_text_test = X_test[\"comment_text\"].str.lower()\n",
    "\n",
    "tk = Tokenizer(num_words=max_features, lower=True)\n",
    "tk.fit_on_texts(raw_text_train)\n",
    "X_train[\"comment_seq\"] = tk.texts_to_sequences(raw_text_train)\n",
    "X_valid[\"comment_seq\"] = tk.texts_to_sequences(raw_text_valid)\n",
    "X_test[\"comment_seq\"] = tk.texts_to_sequences(raw_text_test)\n",
    "\n",
    "X_train = pad_sequences(X_train.comment_seq, maxlen=max_len)\n",
    "X_valid = pad_sequences(X_valid.comment_seq, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test.comment_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "a40adf8a-b7c9-4087-ac1b-c8b4b106c8c8",
    "_uuid": "318f50f4d988efdaf0eef33a0032b5ac194f98c4"
   },
   "outputs": [],
   "source": [
    "# def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "# embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n",
    "\n",
    "def get_coefs(word, *arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "# Open the file with explicit UTF-8 encoding\n",
    "with open(embedding_path, encoding=\"utf-8\") as f:\n",
    "    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "21cb015a-59c7-499d-a962-216d76ae7d95",
    "_uuid": "fbd7eca5dbea615480b7baeb123754b4a1ebb52b"
   },
   "outputs": [],
   "source": [
    "word_index = tk.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "6bb1b099-92dd-4de3-8419-373346608267",
    "_uuid": "600b0aecc8d25edd12f2417eedfc8880dc2b5747"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:34: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m954s\u001b[0m 1s/step - accuracy: 0.8132 - loss: 0.0797 - val_accuracy: 0.9782 - val_loss: 0.0439\n",
      "Epoch 2/3\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1126s\u001b[0m 2s/step - accuracy: 0.9017 - loss: 0.0386 - val_accuracy: 0.9565 - val_loss: 0.0423\n",
      "Epoch 3/3\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1066s\u001b[0m 1s/step - accuracy: 0.8799 - loss: 0.0313 - val_accuracy: 0.8342 - val_loss: 0.0460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x212bf3c6590>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D, Embedding, SpatialDropout1D, Dense, GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "#file_path = r\"C:\\Users\\Lenovo\\projects\\toxic-comments\\bestmodel.h5\"  \n",
    "\n",
    "#check_point = ModelCheckpoint(file_path, monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "\n",
    "def build_model(lr=0.0, lr_d=0.0, units=0, dr=0.0):\n",
    "    inp = Input(shape=(max_len,))\n",
    "    x = Embedding(max_features, embed_size, trainable=False)(inp)\n",
    "    x = SpatialDropout1D(dr)(x)\n",
    "\n",
    "    x = Bidirectional(GRU(units, return_sequences=True))(x)\n",
    "    x = Conv1D(64, kernel_size=2, padding=\"valid\", kernel_initializer=\"he_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    x = concatenate([avg_pool, max_pool])\n",
    "\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.layers[1].set_weights([embedding_matrix])\n",
    "    model.layers[1].trainable = False  # To freeze the embedding layer\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(learning_rate=lr, decay=lr_d), metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_model(lr=0.001, lr_d=0, units=128, dr=0.2)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, Y_train, batch_size=128, epochs=3, validation_data=(X_valid, Y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m998/998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 82ms/step\n"
     ]
    }
   ],
   "source": [
    "# Use the trained model for predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m998/998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 80ms/step - accuracy: 0.9756 - loss: 0.0457\n",
      "Loss: 0.04685152694582939, Accuracy: 0.9755914211273193\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy and other metrics\n",
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Precision: 0.8007549442918062, Recall: 0.7257996926945104, F1 Score: 0.7602540843745648\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Convert predictions and ground truth to binary\n",
    "Y_pred_binary = np.round(predictions)\n",
    "Y_test_binary = Y_test.astype(int)\n",
    "precision = precision_score(Y_test_binary, Y_pred_binary, average='weighted')\n",
    "recall = recall_score(Y_test_binary, Y_pred_binary, average='weighted')\n",
    "f1 = f1_score(Y_test_binary, Y_pred_binary, average='weighted')\n",
    "\n",
    "# Print the metrics\n",
    "print(f\" Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get another confirmation, we tested the model on a separate test dataset from Github. This is not part of the main code, but a re-assurance to ensure the model is not overfitting or specific to the earlier dataset. \n",
    "This dataset has 153165 rows. \n",
    "In this dataset, we got an accuracy of 97.7% which indicates the model is not overfitting, but works well overall for unseen data too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4787/4787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 109ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "test_results = pd.read_csv(r\"C:\\Users\\Lenovo\\projects\\toxic-comments\\datasets\\github\\results.csv\")\n",
    "raw_text_test_results = test_results[\"comment_text\"].str.lower()\n",
    "X_test_results = tk.texts_to_sequences(raw_text_test_results)\n",
    "X_test_results = pad_sequences(X_test_results, maxlen=max_len)\n",
    "\n",
    "# Make predictions on the separate testing dataset\n",
    "Y_pred_results = model.predict(X_test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_11212\\4164531324.py:7: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  test_results[list_classes] = test_results[list_classes].applymap(lambda x: 1 if x > 0.5 else 0)\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_11212\\4164531324.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  Y_pred_results_binary = pd.DataFrame(Y_pred_results).applymap(lambda x: 1 if x > 0.5 else 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4787/4787\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 81ms/step - accuracy: 0.9785 - loss: 0.0318\n",
      "Accuracy on separate testing dataset: 0.9776579141616821\n"
     ]
    }
   ],
   "source": [
    "test_results = pd.read_csv(r\"C:\\Users\\Lenovo\\projects\\toxic-comments\\datasets\\github\\results.csv\")\n",
    "raw_text_test_results = test_results[\"comment_text\"].str.lower()\n",
    "X_test_results = tk.texts_to_sequences(raw_text_test_results)\n",
    "X_test_results = pad_sequences(X_test_results, maxlen=max_len)\n",
    "\n",
    "# Convert dataset values to binary\n",
    "test_results[list_classes] = test_results[list_classes].applymap(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "# Convert predictions to binary\n",
    "Y_pred_results_binary = pd.DataFrame(Y_pred_results).applymap(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "# Evaluate the model using model.evaluate\n",
    "loss, accuracy = model.evaluate(X_test_results, Y_pred_results_binary, verbose=1)\n",
    "print(\"Accuracy on separate testing dataset:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new feature we wish to add is the calculation of a toxicity score along with classification. This score takes all 6 parameters into consideration and calculates a cumulative score which is representative of the overall toxicity of theh model. As of now, this is a simple calculation where each parameter gets equal weightage. But this is something for us to work on in detail in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.6052741e-05 1.0546836e-04 1.0725883e-01 ... 4.9280447e-01 4.6761549e-04\n",
      " 4.5956267e-04]\n"
     ]
    }
   ],
   "source": [
    "# Sum up the predicted probabilities for each class\n",
    "toxicity_score = predictions.sum(axis=1)\n",
    "\n",
    "# Normalize the score to a range of 0 to 1\n",
    "toxicity_score = (toxicity_score - toxicity_score.min()) / (toxicity_score.max() - toxicity_score.min())\n",
    "\n",
    "# Print the toxicity score\n",
    "print(toxicity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "395c5aca-a9f9-4286-8e4f-ee51c7d4b823",
    "_uuid": "777cef77db4b9eb7413d7028331a097be0e29f14"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_pred_binary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Print the predictions, actual values, and toxicity score\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mY_pred_binary\u001b[49m)):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mActual: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mY_test[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Predicted: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mY_pred_binary[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Toxicity Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredictions[i][\u001b[32m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'Y_pred_binary' is not defined"
     ]
    }
   ],
   "source": [
    "# Print the predictions, actual values, and toxicity score\n",
    "for i in range(len(Y_pred_binary)):\n",
    "    print(f\"Actual: {Y_test[i]} - Predicted: {Y_pred_binary[i]} - Toxicity Score: {predictions[i][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Classes: []\n",
      "Toxicity Score: 0.0032990377\n"
     ]
    }
   ],
   "source": [
    "def classify_statement(model, statement):\n",
    "    # Tokenize and pad the input statement\n",
    "    sequence = tk.texts_to_sequences([statement.lower()])\n",
    "    sequence = pad_sequences(sequence, maxlen=max_len)\n",
    "    \n",
    "    # Make a prediction\n",
    "    prediction = model.predict(sequence)[0]\n",
    "    \n",
    "    # Get the toxicity score\n",
    "    toxicity_score = np.mean(prediction)\n",
    "    \n",
    "    # Get the column names where the prediction is 1\n",
    "    columns = np.array([\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])\n",
    "    classes = columns[prediction.round().astype(bool)]\n",
    "    \n",
    "    return classes, toxicity_score\n",
    "\n",
    "# Usage\n",
    "classes, toxicity_score = classify_statement(model, \"Awesome content\")\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Toxicity Score:\", toxicity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Classes: ['toxic' 'obscene' 'insult']\n",
      "Toxicity Score: 0.538155\n"
     ]
    }
   ],
   "source": [
    "classes, toxicity_score = classify_statement(model, \"Dumb shit\")\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Toxicity Score:\", toxicity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "Classes: ['toxic' 'insult' 'identity_hate']\n",
      "Toxicity Score: 0.45556352\n"
     ]
    }
   ],
   "source": [
    "classes, toxicity_score = classify_statement(model, \"you black being\")\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Toxicity Score:\", toxicity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "Classes: ['toxic' 'threat']\n",
      "Toxicity Score: 0.41107512\n"
     ]
    }
   ],
   "source": [
    "classes, toxicity_score = classify_statement(model, \"I will kill you\")\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Toxicity Score:\", toxicity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "Classes: ['toxic' 'obscene' 'insult']\n",
      "Toxicity Score: 0.41269493\n"
     ]
    }
   ],
   "source": [
    "classes, toxicity_score = classify_statement(model, \"The TA was an idiot\")\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Toxicity Score:\", toxicity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
